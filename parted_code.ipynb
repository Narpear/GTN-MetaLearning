{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Data processing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_csv('16_3heads.csv')\n",
    "\n",
    "# Extract embeddings (X) and true_value (y) from the data\n",
    "X = data.drop('True_value', axis=1).values\n",
    "y = data['True_value'].values\n",
    "\n",
    "# Standardize the embeddings (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state = 45)\n",
    "\n",
    "# Convert embeddings and labels to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create custom Dataset for positive and negative pairs\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_emb, anchor_label = self.X[index], self.y[index]\n",
    "        positive_mask = (self.y == anchor_label)\n",
    "        negative_mask = ~positive_mask\n",
    "\n",
    "        # Get positive and negative embeddings and labels\n",
    "        positive_embeddings = self.X[positive_mask]\n",
    "        negative_embeddings = self.X[negative_mask]\n",
    "\n",
    "        # Choose a random positive pair and a random negative pair\n",
    "        pos_idx = np.random.choice(len(positive_embeddings))\n",
    "        neg_idx = np.random.choice(len(negative_embeddings))\n",
    "\n",
    "        pos_emb = positive_embeddings[pos_idx]\n",
    "        neg_emb = negative_embeddings[neg_idx]\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        anchor_emb = torch.tensor(anchor_emb)\n",
    "        pos_emb = torch.tensor(pos_emb)\n",
    "        neg_emb = torch.tensor(neg_emb)\n",
    "\n",
    "        return (anchor_emb, pos_emb), (anchor_emb, neg_emb)\n",
    "\n",
    "# Create DataLoader for training\n",
    "batch_size = 32\n",
    "train_dataset = SiameseDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader for testing\n",
    "test_dataset = SiameseDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Building the Siamese Network\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Siamese Network Module Definition\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prerk\\AppData\\Local\\Temp\\ipykernel_1252\\838715946.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  anchor_emb = torch.tensor(anchor_emb)\n",
      "C:\\Users\\prerk\\AppData\\Local\\Temp\\ipykernel_1252\\838715946.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_emb = torch.tensor(pos_emb)\n",
      "C:\\Users\\prerk\\AppData\\Local\\Temp\\ipykernel_1252\\838715946.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  neg_emb = torch.tensor(neg_emb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 17.31804676055908\n",
      "Epoch 2/200, Train Loss: 12.53757438659668\n",
      "Epoch 3/200, Train Loss: 16.977468490600586\n",
      "Epoch 4/200, Train Loss: 14.552988433837891\n",
      "Epoch 5/200, Train Loss: 15.312651252746582\n",
      "Epoch 6/200, Train Loss: 15.428319931030273\n",
      "Epoch 7/200, Train Loss: 19.067305183410646\n",
      "Epoch 8/200, Train Loss: 15.742814254760741\n",
      "Epoch 9/200, Train Loss: 14.399503707885742\n",
      "Epoch 10/200, Train Loss: 21.151125335693358\n",
      "Epoch 11/200, Train Loss: 14.732491874694825\n",
      "Epoch 12/200, Train Loss: 14.573101425170899\n",
      "Epoch 13/200, Train Loss: 15.431553649902344\n",
      "Epoch 14/200, Train Loss: 14.649923133850098\n",
      "Epoch 15/200, Train Loss: 16.96618309020996\n",
      "Epoch 16/200, Train Loss: 15.137365531921386\n",
      "Epoch 17/200, Train Loss: 17.888256072998047\n",
      "Epoch 18/200, Train Loss: 13.244929695129395\n",
      "Epoch 19/200, Train Loss: 16.418577575683592\n",
      "Epoch 20/200, Train Loss: 15.891202545166015\n",
      "Epoch 21/200, Train Loss: 16.82654151916504\n",
      "Epoch 22/200, Train Loss: 14.0876953125\n",
      "Epoch 23/200, Train Loss: 13.795936584472656\n",
      "Epoch 24/200, Train Loss: 14.710116195678712\n",
      "Epoch 25/200, Train Loss: 15.081686401367188\n",
      "Epoch 26/200, Train Loss: 13.410780334472657\n",
      "Epoch 27/200, Train Loss: 15.948954582214355\n",
      "Epoch 28/200, Train Loss: 16.6604061126709\n",
      "Epoch 29/200, Train Loss: 16.574760055541994\n",
      "Epoch 30/200, Train Loss: 16.8102294921875\n",
      "Epoch 31/200, Train Loss: 14.831350898742675\n",
      "Epoch 32/200, Train Loss: 17.246237373352052\n",
      "Epoch 33/200, Train Loss: 14.4529541015625\n",
      "Epoch 34/200, Train Loss: 16.137988471984862\n",
      "Epoch 35/200, Train Loss: 17.33114242553711\n",
      "Epoch 36/200, Train Loss: 19.38103256225586\n",
      "Epoch 37/200, Train Loss: 15.436641693115234\n",
      "Epoch 38/200, Train Loss: 16.06573429107666\n",
      "Epoch 39/200, Train Loss: 17.82277355194092\n",
      "Epoch 40/200, Train Loss: 17.109546852111816\n",
      "Epoch 41/200, Train Loss: 15.222577476501465\n",
      "Epoch 42/200, Train Loss: 16.441216468811035\n",
      "Epoch 43/200, Train Loss: 14.955806159973145\n",
      "Epoch 44/200, Train Loss: 16.583770179748534\n",
      "Epoch 45/200, Train Loss: 14.11713809967041\n",
      "Epoch 46/200, Train Loss: 19.71512565612793\n",
      "Epoch 47/200, Train Loss: 17.085130310058595\n",
      "Epoch 48/200, Train Loss: 13.624413681030273\n",
      "Epoch 49/200, Train Loss: 13.915677070617676\n",
      "Epoch 50/200, Train Loss: 15.376268005371093\n",
      "Epoch 51/200, Train Loss: 15.60434627532959\n",
      "Epoch 52/200, Train Loss: 15.04055061340332\n",
      "Epoch 53/200, Train Loss: 16.711679649353027\n",
      "Epoch 54/200, Train Loss: 15.00617504119873\n",
      "Epoch 55/200, Train Loss: 15.013919258117676\n",
      "Epoch 56/200, Train Loss: 15.786888885498048\n",
      "Epoch 57/200, Train Loss: 16.02365074157715\n",
      "Epoch 58/200, Train Loss: 14.228056907653809\n",
      "Epoch 59/200, Train Loss: 14.190696525573731\n",
      "Epoch 60/200, Train Loss: 16.52757873535156\n",
      "Epoch 61/200, Train Loss: 16.034852981567383\n",
      "Epoch 62/200, Train Loss: 16.111241149902344\n",
      "Epoch 63/200, Train Loss: 17.055181884765624\n",
      "Epoch 64/200, Train Loss: 15.080318832397461\n",
      "Epoch 65/200, Train Loss: 18.19049491882324\n",
      "Epoch 66/200, Train Loss: 15.5241117477417\n",
      "Epoch 67/200, Train Loss: 16.574850845336915\n",
      "Epoch 68/200, Train Loss: 14.91917667388916\n",
      "Epoch 69/200, Train Loss: 19.660082626342774\n",
      "Epoch 70/200, Train Loss: 17.045594215393066\n",
      "Epoch 71/200, Train Loss: 15.609643363952637\n",
      "Epoch 72/200, Train Loss: 18.086084175109864\n",
      "Epoch 73/200, Train Loss: 15.317920303344726\n",
      "Epoch 74/200, Train Loss: 15.356303215026855\n",
      "Epoch 75/200, Train Loss: 14.586863708496093\n",
      "Epoch 76/200, Train Loss: 13.998290061950684\n",
      "Epoch 77/200, Train Loss: 17.253070831298828\n",
      "Epoch 78/200, Train Loss: 17.531542205810545\n",
      "Epoch 79/200, Train Loss: 12.693515396118164\n",
      "Epoch 80/200, Train Loss: 16.486381340026856\n",
      "Epoch 81/200, Train Loss: 15.53915023803711\n",
      "Epoch 82/200, Train Loss: 16.510447692871093\n",
      "Epoch 83/200, Train Loss: 13.572721099853515\n",
      "Epoch 84/200, Train Loss: 15.533057498931885\n",
      "Epoch 85/200, Train Loss: 17.877518272399904\n",
      "Epoch 86/200, Train Loss: 14.80911922454834\n",
      "Epoch 87/200, Train Loss: 15.934367370605468\n",
      "Epoch 88/200, Train Loss: 16.899338150024413\n",
      "Epoch 89/200, Train Loss: 12.33277931213379\n",
      "Epoch 90/200, Train Loss: 17.47868881225586\n",
      "Epoch 91/200, Train Loss: 16.097095680236816\n",
      "Epoch 92/200, Train Loss: 16.365731620788573\n",
      "Epoch 93/200, Train Loss: 16.300847434997557\n",
      "Epoch 94/200, Train Loss: 16.981554222106933\n",
      "Epoch 95/200, Train Loss: 15.550009727478027\n",
      "Epoch 96/200, Train Loss: 17.838648796081543\n",
      "Epoch 97/200, Train Loss: 16.207102012634277\n",
      "Epoch 98/200, Train Loss: 15.755225372314452\n",
      "Epoch 99/200, Train Loss: 14.241179656982421\n",
      "Epoch 100/200, Train Loss: 15.091328430175782\n",
      "Epoch 101/200, Train Loss: 18.29699764251709\n",
      "Epoch 102/200, Train Loss: 15.349541091918946\n",
      "Epoch 103/200, Train Loss: 15.405232429504395\n",
      "Epoch 104/200, Train Loss: 16.851925468444826\n",
      "Epoch 105/200, Train Loss: 16.913177490234375\n",
      "Epoch 106/200, Train Loss: 13.018221473693847\n",
      "Epoch 107/200, Train Loss: 17.231044578552247\n",
      "Epoch 108/200, Train Loss: 14.987919235229493\n",
      "Epoch 109/200, Train Loss: 14.367871856689453\n",
      "Epoch 110/200, Train Loss: 15.831359672546387\n",
      "Epoch 111/200, Train Loss: 16.628908538818358\n",
      "Epoch 112/200, Train Loss: 14.93150463104248\n",
      "Epoch 113/200, Train Loss: 15.191297340393067\n",
      "Epoch 114/200, Train Loss: 17.111942291259766\n",
      "Epoch 115/200, Train Loss: 15.633311080932618\n",
      "Epoch 116/200, Train Loss: 16.412307357788087\n",
      "Epoch 117/200, Train Loss: 14.965483283996582\n",
      "Epoch 118/200, Train Loss: 17.42847137451172\n",
      "Epoch 119/200, Train Loss: 14.510519027709961\n",
      "Epoch 120/200, Train Loss: 16.098770713806154\n",
      "Epoch 121/200, Train Loss: 15.691219711303711\n",
      "Epoch 122/200, Train Loss: 14.741808700561524\n",
      "Epoch 123/200, Train Loss: 15.683832550048828\n",
      "Epoch 124/200, Train Loss: 16.16992816925049\n",
      "Epoch 125/200, Train Loss: 14.821919631958007\n",
      "Epoch 126/200, Train Loss: 15.007902717590332\n",
      "Epoch 127/200, Train Loss: 16.39580535888672\n",
      "Epoch 128/200, Train Loss: 15.771922302246093\n",
      "Epoch 129/200, Train Loss: 17.52538299560547\n",
      "Epoch 130/200, Train Loss: 14.950010871887207\n",
      "Epoch 131/200, Train Loss: 14.327813720703125\n",
      "Epoch 132/200, Train Loss: 14.970110321044922\n",
      "Epoch 133/200, Train Loss: 16.353649711608888\n",
      "Epoch 134/200, Train Loss: 18.26100902557373\n",
      "Epoch 135/200, Train Loss: 12.836062240600587\n",
      "Epoch 136/200, Train Loss: 17.088352584838866\n",
      "Epoch 137/200, Train Loss: 15.377841758728028\n",
      "Epoch 138/200, Train Loss: 17.14480495452881\n",
      "Epoch 139/200, Train Loss: 16.594409942626953\n",
      "Epoch 140/200, Train Loss: 15.629426002502441\n",
      "Epoch 141/200, Train Loss: 12.770167541503906\n",
      "Epoch 142/200, Train Loss: 13.341850852966308\n",
      "Epoch 143/200, Train Loss: 15.061971473693848\n",
      "Epoch 144/200, Train Loss: 17.626171684265138\n",
      "Epoch 145/200, Train Loss: 14.81525535583496\n",
      "Epoch 146/200, Train Loss: 14.676623344421387\n",
      "Epoch 147/200, Train Loss: 15.012331008911133\n",
      "Epoch 148/200, Train Loss: 16.26316547393799\n",
      "Epoch 149/200, Train Loss: 16.132942962646485\n",
      "Epoch 150/200, Train Loss: 15.966496086120605\n",
      "Epoch 151/200, Train Loss: 13.999100685119629\n",
      "Epoch 152/200, Train Loss: 19.926003837585448\n",
      "Epoch 153/200, Train Loss: 18.876172256469726\n",
      "Epoch 154/200, Train Loss: 13.078752899169922\n",
      "Epoch 155/200, Train Loss: 14.068263244628906\n",
      "Epoch 156/200, Train Loss: 14.151318740844726\n",
      "Epoch 157/200, Train Loss: 14.16995964050293\n",
      "Epoch 158/200, Train Loss: 12.471068191528321\n",
      "Epoch 159/200, Train Loss: 14.539546012878418\n",
      "Epoch 160/200, Train Loss: 16.93635711669922\n",
      "Epoch 161/200, Train Loss: 13.007790851593018\n",
      "Epoch 162/200, Train Loss: 18.055153846740723\n",
      "Epoch 163/200, Train Loss: 12.16402759552002\n",
      "Epoch 164/200, Train Loss: 15.149759483337402\n",
      "Epoch 165/200, Train Loss: 16.006777572631837\n",
      "Epoch 166/200, Train Loss: 15.6986909866333\n",
      "Epoch 167/200, Train Loss: 17.38138484954834\n",
      "Epoch 168/200, Train Loss: 15.56475009918213\n",
      "Epoch 169/200, Train Loss: 13.240272521972656\n",
      "Epoch 170/200, Train Loss: 14.489799690246581\n",
      "Epoch 171/200, Train Loss: 13.537714958190918\n",
      "Epoch 172/200, Train Loss: 15.35732650756836\n",
      "Epoch 173/200, Train Loss: 17.9472074508667\n",
      "Epoch 174/200, Train Loss: 16.753479385375975\n",
      "Epoch 175/200, Train Loss: 12.590703964233398\n",
      "Epoch 176/200, Train Loss: 14.126590538024903\n",
      "Epoch 177/200, Train Loss: 16.22292366027832\n",
      "Epoch 178/200, Train Loss: 13.735008144378662\n",
      "Epoch 179/200, Train Loss: 13.551571655273438\n",
      "Epoch 180/200, Train Loss: 13.85596523284912\n",
      "Epoch 181/200, Train Loss: 14.182888412475586\n",
      "Epoch 182/200, Train Loss: 16.149267196655273\n",
      "Epoch 183/200, Train Loss: 16.644643783569336\n",
      "Epoch 184/200, Train Loss: 14.059358406066895\n",
      "Epoch 185/200, Train Loss: 14.872462463378906\n",
      "Epoch 186/200, Train Loss: 16.78826160430908\n",
      "Epoch 187/200, Train Loss: 16.086930084228516\n",
      "Epoch 188/200, Train Loss: 13.059070014953614\n",
      "Epoch 189/200, Train Loss: 13.227569961547852\n",
      "Epoch 190/200, Train Loss: 14.412182426452636\n",
      "Epoch 191/200, Train Loss: 15.105169486999511\n",
      "Epoch 192/200, Train Loss: 14.611025619506837\n",
      "Epoch 193/200, Train Loss: 14.95228271484375\n",
      "Epoch 194/200, Train Loss: 18.160740470886232\n",
      "Epoch 195/200, Train Loss: 14.924452686309815\n",
      "Epoch 196/200, Train Loss: 17.990506172180176\n",
      "Epoch 197/200, Train Loss: 13.860154724121093\n",
      "Epoch 198/200, Train Loss: 17.203733062744142\n",
      "Epoch 199/200, Train Loss: 12.487709999084473\n",
      "Epoch 200/200, Train Loss: 15.898380661010743\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Training the Siamese Network\n",
    "import torch.optim as optim\n",
    "\n",
    "# Custom contrastive loss with Euclidean distance\n",
    "class ContrastiveLossEuclidean(torch.nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLossEuclidean, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, pos_distance, neg_distance):\n",
    "        loss = (pos_distance).pow(2).mean() + F.relu(self.margin - neg_distance).pow(2).mean()\n",
    "        return loss\n",
    "\n",
    "# Create the Siamese Network\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 8  # Adjust the hidden dimension as needed\n",
    "output_dim = 16 # The final dimension of the learned representations\n",
    "siamese_net = SiameseNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Initialize the custom contrastive loss with Euclidean distance\n",
    "margin = 0.5  # You can adjust this margin based on your data and task\n",
    "criterion = ContrastiveLossEuclidean(margin)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "optimizer = optim.Adam(siamese_net.parameters(), lr=0.001)\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        pairs_pos, pairs_neg = batch\n",
    "        anchors_pos, pos_emb = pairs_pos\n",
    "        anchors_neg, neg_emb = pairs_neg\n",
    "\n",
    "        anchor_output_pos = siamese_net(anchors_pos)\n",
    "        anchor_output_neg = siamese_net(anchors_neg)\n",
    "        pos_distance = F.pairwise_distance(anchor_output_pos, pos_emb, p=2)\n",
    "        neg_distance = F.pairwise_distance(anchor_output_neg, neg_emb, p=2)\n",
    "\n",
    "        loss = criterion(pos_distance, neg_distance)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Calculate testing accuracy at each epoch\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            pairs_pos, pairs_neg = batch\n",
    "            anchors_pos, pos_emb = pairs_pos\n",
    "            anchors_neg, neg_emb = pairs_neg\n",
    "\n",
    "            anchor_output_pos = siamese_net(anchors_pos)\n",
    "            anchor_output_neg = siamese_net(anchors_neg)\n",
    "            pos_distance = F.pairwise_distance(anchor_output_pos, pos_emb, p=2)\n",
    "            neg_distance = F.pairwise_distance(anchor_output_neg, neg_emb, p=2)\n",
    "\n",
    "            pos_similarity = F.pairwise_distance(anchor_output_pos, pos_emb, p=2)\n",
    "            neg_similarity = F.pairwise_distance(anchor_output_neg, neg_emb, p=2)\n",
    "\n",
    "            correct_test += ((pos_similarity < margin).sum() + (neg_similarity >= margin).sum()).item()\n",
    "            total_test += len(pos_similarity) + len(neg_similarity)\n",
    "\n",
    "        test_accuracy = correct_test / total_test\n",
    "        test_accuracies.append(test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200, Test Accuracy: 0.5031847133757962\n",
      "Confusion Matrix:\n",
      "[[155   2]\n",
      " [  3 154]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prerk\\AppData\\Local\\Temp\\ipykernel_1252\\838715946.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  anchor_emb = torch.tensor(anchor_emb)\n",
      "C:\\Users\\prerk\\AppData\\Local\\Temp\\ipykernel_1252\\838715946.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_emb = torch.tensor(pos_emb)\n",
      "C:\\Users\\prerk\\AppData\\Local\\Temp\\ipykernel_1252\\838715946.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  neg_emb = torch.tensor(neg_emb)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 4: Confusion Matrix for the true and predicted values\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Testing loop (evaluate on the test dataset) for the last epoch\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        pairs_pos, pairs_neg = batch\n",
    "        anchors_pos, pos_emb = pairs_pos\n",
    "        anchors_neg, neg_emb = pairs_neg\n",
    "\n",
    "        anchor_output_pos = siamese_net(anchors_pos)\n",
    "        anchor_output_neg = siamese_net(anchors_neg)\n",
    "        pos_distance = F.pairwise_distance(anchor_output_pos, pos_emb, p=2)\n",
    "        neg_distance = F.pairwise_distance(anchor_output_neg, neg_emb, p=2)\n",
    "\n",
    "        # Calculate test accuracy\n",
    "        pos_similarity = F.pairwise_distance(anchor_output_pos, pos_emb, p=2)\n",
    "        neg_similarity = F.pairwise_distance(anchor_output_neg, neg_emb, p=2)\n",
    "\n",
    "        correct_test += ((pos_similarity < margin).sum() + (neg_similarity >= margin).sum()).item()\n",
    "        total_test += len(pos_similarity) + len(neg_similarity)\n",
    "\n",
    "        # Store predicted and true labels for the last epoch\n",
    "        predicted_labels.extend((pos_similarity >= margin).cpu().numpy().tolist())\n",
    "        predicted_labels.extend((neg_similarity < margin).cpu().numpy().tolist())\n",
    "        true_labels.extend([1] * len(pos_similarity))\n",
    "        true_labels.extend([0] * len(neg_similarity))\n",
    "\n",
    "test_accuracy = correct_test / total_test\n",
    "test_accuracies.append(test_accuracy)\n",
    "print(f\"Epoch {num_epochs}/{num_epochs}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Graphical representation of confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "314\n",
      "[0 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1\n",
      " 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 1 0\n",
      " 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 1\n",
      " 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1\n",
      " 0 0 1 1 1 0 1 1 1]\n",
      "157\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "314\n"
     ]
    }
   ],
   "source": [
    "print(predicted_labels)\n",
    "print(len(predicted_labels))\n",
    "\n",
    "# print(y_test)\n",
    "# print(len(y_test))\n",
    "\n",
    "print(true_labels)\n",
    "print(len(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9840764331210191\n",
      "Precision: 0.9871794871794872\n",
      "Recall: 0.9808917197452229\n",
      "F1 Score: 0.9840255591054313\n",
      "Area Under Curve (AUC): 0.9840764331210191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "# Convert the predicted labels and true labels to numpy arrays\n",
    "predicted_labels_np = np.array(predicted_labels)\n",
    "true_labels_np = np.array(true_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels_np, predicted_labels_np)\n",
    "precision = precision_score(true_labels_np, predicted_labels_np)\n",
    "recall = recall_score(true_labels_np, predicted_labels_np)\n",
    "f1 = f1_score(true_labels_np, predicted_labels_np)\n",
    "auc = roc_auc_score(true_labels_np, predicted_labels_np)\n",
    "\n",
    "print(\"Testing Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Area Under Curve (AUC):\", auc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
